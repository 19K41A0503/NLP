# -*- coding: utf-8 -*-
"""finally.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17kDotD9M666MGHX4x01L1yqclXHDPJIn
"""

import numpy as np
import pandas as pd
import nltk
import seaborn as sns
import matplotlib.pyplot as plt
from keras.models import Sequential
from keras.layers import Dense, LSTM, SimpleRNN, Embedding
from keras.preprocessing.text import Tokenizer
from keras_preprocessing.sequence import  pad_sequences
from keras.utils.np_utils import to_categorical
from tensorflow.keras.layers import  Dropout,Activation

from google.colab import drive
drive.mount('/content/drive')

!pip install emoji==1.7
import emoji

data = pd.read_csv('/content/drive/MyDrive/emoji/emoji_data.csv', header=None)
data.head()

emoji_dict = {
0: ":red_heart:",
    1: ":baseball:",
    2: ":grinning_face_with_big_eyes:",
    3: ":disappointed_face:",
    4: ":fork_and_knife_with_plate:"
}

def label_to_emoji(label):
    return emoji.emojize(emoji_dict[label])

import string
import re

def tweet_clean(tweet):
    tweet = str(tweet).lower()
    rm_mention = re.sub(r'@[A-Za-z0-9]+', '', tweet)                       # remove @mentions
    rm_rt = re.sub(r'RT[/s]+', '', rm_mention)                             # remove RT
    rm_links = re.sub(r'http\S+', '', rm_rt)                               # remove hyperlinks
    rm_links = re.sub(r'https?:\/\/\S+','', rm_links)
    rm_nums = re.sub('[0-9]+', '', rm_links)                        # remove numbers
    rm_punc = [char for char in rm_nums if char not in string.punctuation] # remove punctuations
    rm_punc = ''.join(rm_punc)
    cleaned = rm_punc
    
    return cleaned

data[0]= data[0].apply(tweet_clean)
data.head()

sns.countplot(data[1])

X= data[0].values
Y= data[1].values

file = open('/content/drive/MyDrive/glove.6B.100d.txt', 'r', encoding = 'utf8')
content = file.readlines()
file.close()

embeddings = {}

for line in content:

    line = line.split()
    embeddings[line[0]] = np.array(line[1:], dtype = float)

def get_maxlen(train_data):
    maxlen = 0
    for sent in train_data:
        maxlen = max(maxlen, len(sent))
    return maxlen

maxlen = get_maxlen(Xtokens)
print(maxlen)

tokenizer = Tokenizer()
tokenizer.fit_on_texts(X)
word2index = tokenizer.word_index

Xtokens = tokenizer.texts_to_sequences(X)

Xtrain = pad_sequences(Xtokens, maxlen = maxlen,  padding = 'post', truncating = 'post')

Ytrain = to_categorical(Y)

embed_size = 100
embedding_matrix = np.zeros((len(word2index)+1, embed_size))

for word, i in word2index.items():
    embed_vector = embeddings[word]
    embedding_matrix[i] = embed_vector

model = Sequential([
    Embedding(input_dim = len(word2index) + 1,
              output_dim = embed_size,
              input_length = maxlen,
              weights = [embedding_matrix],
              trainable = False
             ),
    LSTM(units = 32, return_sequences = True),
    Dropout(0.3),
    LSTM(units = 16),
    Dropout(0.2),
    Dense(5, activation = 'softmax')

    
])

model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])

history=model.fit(Xtrain, Ytrain,validation_split = 0.1, batch_size=32, epochs = 50)

import matplotlib.pyplot as plt
import seaborn as sns
get_ipython().run_line_magic('matplotlib', 'inline')
plt.figure(figsize=(8,6))
plt.title(' Accuracy scores')
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.legend(['accuracy', 'val_accuracy'])
plt.show()
plt.figure(figsize=(8,6))
plt.title(' Loss value')
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.legend(['loss', 'val_loss'])
plt.show()

test_data=pd.read_csv('/content/drive/MyDrive/emoji/test_emoji.csv', header=None)
test_data.head()

test_data[0]=test_data[0].apply(lambda x:x[:-1])
test_data.head()

x_test=test_data[0].values
y_test=test_data[1].values

test = ["i love eating food","food","i love you","stop shouting","i have a ball"]

test_seq = tokenizer.texts_to_sequences(test)
Xtest = pad_sequences(test_seq, maxlen = maxlen, padding = 'post', truncating = 'post')

y_pred = model.predict(Xtest)
y_pred = np.argmax(y_pred, axis = 1)

for i in range(len(test)):
    print(test[i], label_to_emoji(y_pred[i]))